## üí• TL;DR: Which is Faster?

**SQL Joins** (RDBMS like MySQL/PostgreSQL) tend to be faster and more optimized for joins‚Äîbecause they are designed with normalized data and relational operations in mind.

**MongoDB Aggregation** can be faster in certain denormalized scenarios, but joins (via `$lookup`) are generally heavier than in SQL.

---

### üß© 1. SQL Joins

- Highly optimized for joining across multiple tables.
- Uses indexes and query planners to optimize performance.
- **Types**: `INNER JOIN`, `LEFT JOIN`, `RIGHT JOIN`, `FULL OUTER JOIN`.

#### üìå Example:

```sql
SELECT o.id, c.name
FROM orders o
JOIN customers c ON o.customer_id = c.id;
```

#### ‚úÖ Best For:

- Normalized data
- Complex relationships
- Multi-table queries

---

### üîÑ 2. MongoDB Aggregation with `$lookup`

- MongoDB is not optimized for joins but supports them using `$lookup` in the aggregation pipeline.
- Works better with denormalized data.
- Joins can get expensive, especially on large datasets or unindexed fields.

#### üìå Example:

```javascript
db.orders.aggregate([
    {
        $lookup: {
            from: "customers",
            localField: "customer_id",
            foreignField: "_id",
            as: "customer_info"
        }
    }
]);
```

#### ‚úÖ Best For:

- Denormalized or embedded documents
- Simple one-to-few lookups
- Data where joins are rare

---

### ‚ö° Performance Factors

| **Factor**            | **SQL Joins**         | **MongoDB Aggregation ($lookup)** |
|------------------------|-----------------------|------------------------------------|
| Join optimization      | ‚úÖ Very strong        | ‚ùå Not as optimized               |
| Index support          | ‚úÖ Strong            | ‚ö†Ô∏è Must manually ensure indexes   |
| Speed on large data    | ‚úÖ Often faster      | ‚ùå Slower if documents are large   |
| Query planner          | ‚úÖ Yes              | ‚ö†Ô∏è Basic                          |
| Complexity handling    | ‚úÖ Strong           | ‚ö†Ô∏è Complex pipelines get slow     |

---

### üß† Interview Answer Example

"In general, SQL databases like PostgreSQL or MySQL handle joins more efficiently because they're optimized for relational data. They use indexes and advanced query planners to minimize overhead. MongoDB supports joins through `$lookup`, but it's typically less efficient‚Äîespecially on large collections‚Äîbecause it's more designed for denormalized data. If performance is a concern and you're doing frequent joins, an RDBMS is usually the better choice. However, if you're working with embedded documents and rare joins, MongoDB can perform very well."


## Step-by-Step: How PostgreSQL Stores Data in a B+ Tree Index

### üß© Use Case Setup

Suppose we have a PostgreSQL table:

```sql
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    username TEXT,
    email TEXT
);
```

Now, create an index on `username`:

```sql
CREATE INDEX idx_username ON users(username);
```

This builds a B+ Tree index on the `username` column.

---

### üß± How PostgreSQL Stores and Uses a B+ Tree Index

#### üîπ Step 1: Index Creation ‚Äî Initial State

- PostgreSQL allocates a root node for the B+ Tree.
- Starts as a leaf node (empty).
- Each node is a page/block (8 KB), holding many entries.

#### üîπ Step 2: First Insertions ‚Üí Leaf Node Builds

Insert some usernames:

```sql
INSERT INTO users (username) VALUES
('adam'), ('ben'), ('carol'), ('david'), ('emma');
```

The leaf node now contains:

```
[adam ‚Üí TID] [ben ‚Üí TID] [carol ‚Üí TID] [david ‚Üí TID] [emma ‚Üí TID]
```

- Each entry: key (`username`) + TID (Tuple ID: page number, tuple index)
- Keys are kept **sorted**!

#### üîπ Step 3: Leaf Node Splits (Overflow)

Insert more usernames:

```sql
INSERT INTO users (username)
VALUES ('fred'), ('george'), ('harry'), ('irene'), ('jack'), ('karen');
```

When the leaf node exceeds 8 KB, it splits:

**Before split:**
```
[adam, ben, carol, david, emma, fred, george, harry, irene, jack, karen]
```

**After split:**
```
Leaf 1: [adam, ben, carol, david, emma]
Leaf 2: [fred, george, harry, irene, jack, karen]
```

A new root node is created:

```
                [fred]
             /     \
[adam...emma] [fred...karen]
```

- Root contains separating key for navigation.

#### üîπ Step 4: More Inserts ‚Üí Internal Node Splits

With more usernames (e.g., 1000+):

- Leaf nodes split into siblings.
- Internal nodes manage navigation.
- All leaf nodes are linked left ‚Üí right.

**Example structure:**
```
                            [gomez, nancy]
                         /     |      \
             [a-f]   [g-m]    [n-z]
```
- Each node = page in memory/disk.

#### üîπ Step 5: Range Query Traversal

For a range query:

```sql
SELECT * FROM users WHERE username BETWEEN 'david' AND 'harry';
```

- Start at root.
- Navigate to correct child node (binary search).
- Land at leaf node containing 'david'.
- Scan rightward through linked leaves for relevant usernames.

> **B+ Trees are perfect for range scans!**

#### üîπ Step 6: Index Lookup for WHERE Clause

For a point lookup:

```sql
SELECT * FROM users WHERE username = 'karen';
```

- Traverse root ‚Üí internal ‚Üí leaf via key comparisons.
- Find 'karen' ‚Üí TID.
- Use TID to fetch the actual row.

---

### üß† Summary: PostgreSQL B+ Tree Storage

| Concept            | Description                                         |
|--------------------|-----------------------------------------------------|
| Internal nodes     | Only keys (e.g., 'gomez') for routing               |
| Leaf nodes         | (key ‚Üí TID) mappings, sorted and linked             |
| TID                | Tuple Identifier ‚Üí points to actual row in heap     |
| Splits             | Leaf/internal nodes split if full                   |
| Tree growth        | Self-balancing, shallow (log N depth)               |
| Range query eff.   | High, due to linked leaf nodes                      |
| Index scan perf.   | O(log n) lookup, O(n) range scan                    |

---

## How B+ Tree Indexes Are Stored on Disk in PostgreSQL

### üíæ 1. B+ Tree Nodes Are Disk Pages

- Nodes are 8 KB disk pages (blocks).
- Each page holds many keys.
- Fewer pages = fewer disk reads.
- Each node = small file chunk on disk, mapped to page numbers.

### üìö 2. Hierarchical Fan-out Reduces Tree Depth

- Internal nodes store hundreds of keys ‚Üí shallow tree.
- For millions of rows, tree is usually 2‚Äì4 levels deep.

**Example (1 million rows, 100 keys/node):**
```
Level 0 (root):     1 node
Level 1:          ~100 nodes
Level 2 (leaves): ~10,000 leaf pages
```
- Only 2‚Äì3 disk I/Os needed to find a value!

### üìÇ 3. Leaf Nodes Are Linked for Fast Range Reads

- All leaf nodes are doubly linked.
- For range queries, DB seeks first match, scans rightward through leaves.
- Sequential reads are faster than random reads on disk.

### üîÅ 4. PostgreSQL Uses a Buffer Cache

- Shared buffer pool in memory.
- Frequently accessed pages (root, hot keys, internal nodes) cached in RAM.
- Many lookups happen in memory, not on disk.
- Root page is almost always cached.

### üì¶ 5. Clustered Index (for Primary Keys)

- Primary key index can be clustered using `CLUSTER`.
- Physical table rows follow B+ Tree order on disk.
- Index range scans read rows sequentially, reducing I/O.
- **Note:** `CLUSTER` must be run manually.

### üìä 6. Disk I/O Patterns: B+ Tree vs Heap Scan

| Operation           | B+ Tree Index Scan         | Heap/Table Scan      |
|---------------------|---------------------------|----------------------|
| Disk I/O (random)   | Few (2‚Äì4)                 | Many (full scan)     |
| Data locality       | Good (leaf links)         | Poor                 |
| Point lookup        | Fast (O(log n))           | Slow (O(n))          |
| Range query         | Fast (sequential)         | Slow                 |
| Cache effectiveness | High                      | Low                  |



# Let's walk through how an index table looks in PostgreSQL using a B+ Tree, and how searching using the index is much more efficient than scanning the entire table.

## üß© Example Table

Suppose we have a table:

```sql
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    username TEXT,
    email TEXT
);
```

And we insert 10 users:

```sql
INSERT INTO users (username, email)
VALUES 
    ('alice', 'a@example.com'),
    ('bob', 'b@example.com'),
    ('carol', 'c@example.com'),
    ('david', 'd@example.com'),
    ('emma', 'e@example.com'),
    ('fred', 'f@example.com'),
    ('george', 'g@example.com'),
    ('harry', 'h@example.com'),
    ('irene', 'i@example.com'),
    ('jack', 'j@example.com');
```

Then we create an index:

```sql
CREATE INDEX idx_username ON users(username);
```

## üì¶ What Does the Index Table Look Like?

The index is a separate structure from the table. It contains:

| username | TID (Tuple ID) |
|----------|---------------|
| alice    | (1,1)         |
| bob      | (1,2)         |
| carol    | (1,3)         |
| david    | (1,4)         |
| emma     | (1,5)         |
| fred     | (1,6)         |
| george   | (1,7)         |
| harry    | (1,8)         |
| irene    | (1,9)         |
| jack     | (1,10)        |

> **TID** = (page number, row index) in PostgreSQL's heap table.

## üìö Internally Stored as a B+ Tree

Imagine the B+ Tree like this (simplified):

```
                [emma]
             /     \
[alice ... david]   [emma ... jack]
```

Each leaf node contains entries like:

```
[fred ‚Üí (1,6)], [george ‚Üí (1,7)], ...
```

All leaf nodes are linked.

## üîç Example Query: Without Index

```sql
SELECT * FROM users WHERE username = 'harry';
```

‚ùå **Without index (sequential scan):**

- PostgreSQL checks each row from top to bottom.
- **Time:** O(n) (linear scan).

## ‚úÖ With Index

With `idx_username`:

- PostgreSQL goes to the root of the B+ Tree.
- Follows internal nodes (e.g., is 'harry' < 'emma'? No ‚Üí right branch).
- Reaches leaf node with 'harry' ‚Üí (1,8).
- Uses the TID to jump to exact row in heap.

‚û°Ô∏è **Time:** O(log n) for index lookup + 1 disk read for heap fetch.

## üîÅ Range Query Example

```sql
SELECT * FROM users WHERE username BETWEEN 'david' AND 'irene';
```

üîç **With Index:**

- Traverse tree to 'david'.
- Follow linked leaf nodes: 'david', 'emma', 'fred', 'george', 'harry', 'irene'.
- Fetch all TIDs in order.

‚úÖ Fast because it touches only relevant leaf nodes.



# Denormalization for Fast Read Access

Denormalization is a common pattern in production systems, especially for analytics, reporting, and read-heavy APIs.

---

## Goals

- **Keep normalized tables** for write operations (source of truth).
- **Create denormalized tables** for fast read access.
- **Keep denormalized tables in sync** with normalized ones.

---

## Step-by-Step Strategy

### 1. Design the Denormalized Schema

- Combine data from multiple normalized tables (joins, lookups) into one wide denormalized table.
- Include precomputed fields if needed (e.g., total amounts, counts).

**Example:**

```sql
-- Normalized
users(id, name)
orders(id, user_id, amount)

-- Denormalized
user_orders(id, user_name, order_id, order_amount)
```

---

### 2. Create Materialized Views (If Supported)

If using PostgreSQL or Oracle, you can create materialized views:

```sql
CREATE MATERIALIZED VIEW user_orders AS
SELECT 
    u.id AS user_id,
    u.name AS user_name,
    o.id AS order_id,
    o.amount AS order_amount
FROM users u
JOIN orders o ON u.id = o.user_id;
```

- Use `REFRESH MATERIALIZED VIEW` to update data (manually or via cron job).
- Can be incrementally refreshed with logic or triggers.

---

### 3. Use Triggers for Real-Time Sync

Set up `AFTER INSERT/UPDATE/DELETE` triggers on the base tables.

**Example in PostgreSQL:**

```sql
CREATE OR REPLACE FUNCTION update_user_orders()
RETURNS TRIGGER AS $$
BEGIN
    -- Simplified logic: delete and re-insert affected rows
    DELETE FROM user_orders WHERE order_id = NEW.id;
    INSERT INTO user_orders
    SELECT u.id, u.name, o.id, o.amount
    FROM users u
    JOIN orders o ON u.id = o.user_id AND o.id = NEW.id;
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER sync_user_orders
AFTER INSERT OR UPDATE ON orders
FOR EACH ROW EXECUTE FUNCTION update_user_orders();
```

---

### 4. ETL/ELT Pipeline (Batch Sync)

For more complex or high-volume systems:

- Use a batch job (Node.js, Python, SQL scripts, etc.) that joins the normalized tables and writes the result into the denormalized table.
- Schedule with cron, Airflow, or database jobs.

**Example in Node.js:**

```js
const result = await db.query(`
    INSERT INTO user_orders (user_id, user_name, order_id, order_amount)
    SELECT u.id, u.name, o.id, o.amount
    FROM users u
    JOIN orders o ON u.id = o.user_id
`);
```

---

### 5. Use Logical Replication (for Cross-Instance or Async)

If your denormalized tables live in another database or server:

- Use logical replication (Postgres), Change Data Capture (CDC), or Debezium.
- Sync changes from normalized tables into denormalized read replicas.

---

## Tips

- Keep denormalized tables **read-only** in your app.
- Consider **indexes and partitions** for large denormalized tables.
- If data changes frequently, ensure **consistency tradeoffs** are well-understood.
- Use **timestamps or change flags** for incremental syncs.




# Partitioning vs Sharding in Databases

Partitioning and sharding are techniques to manage large datasets for performance, scalability, and maintainability. Here‚Äôs a concise guide for production-grade systems.

---

## üîπ Partitioning

Partitioning divides a large table into smaller, manageable pieces (partitions) while retaining the logical structure of a single table.

**Benefits:**
- **Faster queries:** Reduces I/O by scanning only relevant partitions.
- **Easier maintenance:** Archive/drop old partitions.
- **Parallel queries:** Queries can run across partitions.
- **Optimized indexes:** Indexes are smaller and more efficient.

### Types of Partitioning

| Type                  | Description                       | Example                                      |
|-----------------------|-----------------------------------|----------------------------------------------|
| Range Partitioning    | By range of column values         | `created_at` monthly partitions              |
| List Partitioning     | By discrete values                | Partition by country (`'US'`, `'IN'`)        |
| Hash Partitioning     | By hash function on a column      | Partition by `user_id % 4`                   |
| Composite Partitioning| Combines methods                  | Range + hash for multi-tenant cases          |

### Example: PostgreSQL Range Partitioning

```sql
-- Step 1: Create partitioned table
CREATE TABLE user_logs (
    id SERIAL PRIMARY KEY,
    user_id INT,
    activity TEXT,
    created_at DATE
) PARTITION BY RANGE (created_at);

-- Step 2: Create partitions
CREATE TABLE user_logs_2024_01 PARTITION OF user_logs
    FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');
CREATE TABLE user_logs_2024_02 PARTITION OF user_logs
    FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');
-- ...more partitions

-- Step 3: Insert/query (no change in logic)
INSERT INTO user_logs (user_id, activity, created_at)
VALUES (1, 'login', '2024-01-25');

SELECT * FROM user_logs
WHERE created_at BETWEEN '2024-01-01' AND '2024-02-01';
```
_PostgreSQL scans only relevant partitions (partition pruning)._

### Production Best Practices

| Concern                | Recommendation                                 |
|------------------------|------------------------------------------------|
| Query performance      | Always filter by partition key                 |
| Auto partition mgmt    | Automate creation/rotation (cron/app logic)    |
| Retention/archiving    | Drop old partitions for instant purging        |
| Indexing               | Index only needed columns per partition        |
| Monitoring             | Track bloat, query plans, partition sizes      |
| Migrations             | Plan changes during downtime or with tools     |

### Tools & Libraries

- **pg_partman:** Automatic partition maintenance for PostgreSQL
- **TimescaleDB:** Advanced time-series partitioning (hypertables)
- **Custom logic:** For NoSQL/ORMs, partitioning at app level

### Partitioning in NoSQL

- **MongoDB:** Sharding (horizontal partitioning)
- **Cassandra:** Partition key defines data locality
- **DynamoDB:** Partition key + sort key

---

## üîπ Sharding

Sharding splits data horizontally across multiple databases or machines (shards), each holding a subset of the data. Used when partitioning a single table isn‚Äôt enough.

**Key Points:**
- **Scales beyond a single server**
- **Each shard is a separate DB/server**
- **Requires routing logic in the app**

### Sharding vs Partitioning

| Feature         | Partitioning                | Sharding                       |
|-----------------|----------------------------|--------------------------------|
| Level           | Within one DB instance      | Across multiple DBs/servers    |
| Goal            | Performance, manageability  | Scalability, capacity          |
| Maintenance     | Easier                      | More complex                   |
| Query engine    | Centralized                 | Often needs routing logic      |

### Example: User-Based Sharding

Suppose a social app with 100M users:

- Users 1‚Äì10M ‚Üí `user_db_1`
- Users 10M‚Äì20M ‚Üí `user_db_2`
- ...each DB on a separate server

#### Sharding Steps

1. **Choose a Shard Key**
   - Distributes data evenly
   - Matches query patterns
   - Is immutable
   - _Good keys:_ `user_id`, `tenant_id`, `region`
   - _Bad keys:_ `email`, `timestamp`

2. **Set Up Multiple Databases**
   ```bash
   user_db_1 (db1.server.com)
   user_db_2 (db2.server.com)
   ```

3. **Routing Layer in App**
   ```typescript
   function getShardForUser(userId) {
     const numShards = 4;
     return `user_db_${userId % numShards}`;
   }
   // Use a connection pool per shard
   ```

4. **Querying/Writing**
   ```typescript
   const db = getShardForUser(userId);
   await db.query("SELECT * FROM users WHERE id = $1", [userId]);
   ```

### Sharding Best Practices

| Concern         | Solution                                 |
|-----------------|------------------------------------------|
| Rebalancing     | Use shard map, re-shard gradually        |
| Fault tolerance | Redundancy per shard (replication)       |
| Monitoring      | Track load per shard                     |
| Backups         | Independent backups per shard            |
| Security        | Encrypt keys, isolate DBs                |
| Deployment      | Use Terraform/K8s for scaling            |

---

## üîö Summary

- **Partitioning:** Breaks large tables into smaller ones for faster queries and easier management.
- **Sharding:** Splits data across multiple DBs/servers for horizontal scaling.

**Production tips:**
- Use range partitioning for time-series logs.
- Automate partition/shard rotation.
- Monitor performance and bloat.
- Plan schema, indexes, and routing logic carefully.
- Avoid cross-shard operations.


## Partitioning vs Sharding: Key Differences

The major difference between partitioning and sharding lies in where and how the data is split.

### üîç At a Glance

| Feature           | Partitioning                                                                 | Sharding                                                      |
|-------------------|------------------------------------------------------------------------------|---------------------------------------------------------------|
| **Definition**    | Dividing a table into smaller parts within the same database instance         | Dividing the data across multiple databases or servers        |
| **Goal**          | Performance & manageability                                                  | Scalability & system capacity                                 |
| **Data Location** | All partitions are on the same server / DB instance                          | Shards live on different servers / DBs                        |
| **Query Handling**| Handled internally by the DBMS (transparent)                                 | App or middleware must route queries to the right shard       |
| **Indexing**      | Indexing can be partition-specific                                           | Each shard has its own indexes                                |
| **Cross-table joins** | Easy, within one DB                                                      | Hard, requires merging across shards                          |
| **Failure Isolation** | Not isolated; all partitions go down if DB crashes                       | Fault is isolated per shard                                   |
| **Scalability**   | Limited by single machine                                                    | Scales horizontally across machines                           |

---

### üéØ Use Cases

#### ‚úÖ Use Partitioning When:
- You're dealing with very large tables (e.g. logs, events, transactions)
- You need to archive old data easily
- You want to optimize queries by date, region, etc.
- You want to improve vacuuming/performance in PostgreSQL
- You're not yet hitting the resource limits of your database server

**Example:**  
A financial app stores millions of transaction logs ‚Äî partition by `transaction_date` monthly to optimize reads & archiving.

#### ‚úÖ Use Sharding When:
- You‚Äôve outgrown a single DB's CPU, RAM, or storage
- You have billions of rows or millions of users
- Your traffic is high enough to need multiple DB servers
- You want fault isolation (e.g., if shard A fails, shard B is unaffected)
- You're building a multi-tenant SaaS (e.g., each tenant in a separate shard)

**Example:**  
A social network with 100M users shards user data by `user_id % N`, each shard lives on a different DB server.

---

### üîö Summary

| Concept        | Partitioning                  | Sharding                        |
|----------------|------------------------------|---------------------------------|
| **Scaling**    | Vertical (within 1 DB)       | Horizontal (across DBs)         |
| **Managed By** | Database engine              | Application or middleware       |
| **Complexity** | Lower                        | Higher                          |
| **Performance**| Improves local query performance | Solves data volume & traffic scaling |

---

### üöÄ Rule of Thumb

> Start with partitioning. Move to sharding when you hit hardware or performance limits that partitioning can't solve.



## Partitioning vs Sharding: Key Differences

The major difference between partitioning and sharding lies in where and how the data is split.

### üîç At a Glance

| Feature            | Partitioning                                                                 | Sharding                                                      |
|--------------------|------------------------------------------------------------------------------|---------------------------------------------------------------|
| **Definition**     | Dividing a table into smaller parts within the same database instance         | Dividing the data across multiple databases or servers         |
| **Goal**           | Performance & manageability                                                  | Scalability & system capacity                                 |
| **Data Location**  | All partitions are on the same server / DB instance                          | Shards live on different servers / DBs                        |
| **Query Handling** | Handled internally by the DBMS (transparent)                                 | App or middleware must route queries to the right shard        |
| **Indexing**       | Indexing can be partition-specific                                           | Each shard has its own indexes                                |
| **Cross-table joins** | Easy, within one DB                                                       | Hard, requires merging across shards                          |
| **Failure Isolation** | Not isolated; all partitions go down if DB crashes                        | Fault is isolated per shard                                   |
| **Scalability**    | Limited by single machine                                                    | Scales horizontally across machines                           |

---

### üéØ Use Cases

#### ‚úÖ Use Partitioning When:
- Dealing with very large tables (e.g. logs, events, transactions)
- Need to archive old data easily
- Want to optimize queries by date, region, etc.
- Want to improve vacuuming/performance in PostgreSQL
- Not yet hitting the resource limits of your database server

**Example:**  
A financial app stores millions of transaction logs ‚Äî partition by `transaction_date` monthly to optimize reads & archiving.

#### ‚úÖ Use Sharding When:
- Outgrown a single DB's CPU, RAM, or storage
- Have billions of rows or millions of users
- Traffic is high enough to need multiple DB servers
- Want fault isolation (e.g., if shard A fails, shard B is unaffected)
- Building a multi-tenant SaaS (e.g., each tenant in a separate shard)

**Example:**  
A social network with 100M users shards user data by `user_id % N`, each shard lives on a different DB server.

---

### üîö Summary

| Concept        | Partitioning                  | Sharding                        |
|----------------|------------------------------|---------------------------------|
| **Scaling**    | Vertical (within 1 DB)        | Horizontal (across DBs)         |
| **Managed By** | Database engine               | Application or middleware       |
| **Complexity** | Lower                         | Higher                          |
| **Performance**| Improves local query performance | Solves data volume & traffic scaling |

---

### üöÄ Rule of Thumb

> Start with partitioning. Move to sharding when you hit hardware or performance limits that partitioning can't solve.



# DELETE and TRUNCATE in SQL

DELETE and TRUNCATE are used in databases (like MySQL, PostgreSQL, SQL Server) to remove data from tables, but they work differently:

### üü† 1. DELETE

‚úÖ **Purpose:**
Removes specific rows from a table, using an optional WHERE clause.

üìå **Key Characteristics:**
- Row-by-row deletion
- Can filter rows with WHERE
- Triggers are fired
- Slower for large data (because each row is logged)
- Transaction-safe ‚Äî can be rolled back

üß† **Example:**

```sql
DELETE FROM users WHERE status = 'inactive';
```

üîπ Only users with status 'inactive' will be deleted. Others remain.

### üü£ 2. TRUNCATE

‚úÖ **Purpose:**
Removes all rows from a table, no filtering allowed.

üìå **Key Characteristics:**
- Cannot use WHERE clause
- Faster than DELETE ‚Äî uses less logging
- Often resets auto-increment counters
- Does not fire ON DELETE triggers (depends on DB)
- Usually non-transactional ‚Äî can't roll back in most DBs

üß† **Example:**

```sql
TRUNCATE TABLE users;
```

üîπ Entire users table is now empty, but structure remains.



# PostgreSQL ALTER TABLE and Metadata Storage Guide(let suppose i have to add a column in a table which has 10 millions row  in sql what would be the difference if i run alter table set age or alter table set age default value 24)

## Adding Columns to Large Tables (10M+ rows)

### 1. `ALTER TABLE ADD age INT;`

**Effect:** Adds the column `age` with `NULL` as the default for existing rows.

**Performance:** ‚ö° Very fast, even on large tables
- Metadata-only operation in PostgreSQL
- No data rewrite occurs
- Instantaneous regardless of table size

### 2. `ALTER TABLE ADD age INT DEFAULT 24;`

There are two scenarios depending on whether `NOT NULL` is specified:

#### Case A: Just Default (No NOT NULL)
```sql
ALTER TABLE my_table ADD age INT DEFAULT 24;
```
- **Effect:** New column added, default is 24 for new inserts, but existing rows still have `NULL`
- **Performance:** ‚ö° Still fast ‚Äî no data rewrite needed
- **Existing rows:** Still `NULL` unless `NOT NULL` is also added

#### Case B: Default + NOT NULL
```sql
ALTER TABLE my_table ADD age INT DEFAULT 24 NOT NULL;
```
- **Effect:** Column added, all existing 10 million rows will be physically updated to store 24
- **Performance:** üêå Slow and heavy ‚Äî table rewrite required
- **Duration:** Can take minutes to hours depending on disk speed, indexing, constraints, etc.

## Summary Table

| Command | Existing Rows Value | Performance on 10M rows |
|---------|-------------------|------------------------|
| `ADD age INT` | `NULL` | ‚ö° Fast (metadata-only) |
| `ADD age INT DEFAULT 24` | `NULL` | ‚ö° Fast |
| `ADD age INT DEFAULT 24 NOT NULL` | 24 | üêå Slow (full table rewrite) |

## How PostgreSQL Handles These Operations Internally

### Metadata-Only Changes
- PostgreSQL does **not** rewrite the table
- Updates only internal system catalogs (PostgreSQL's internal bookkeeping)
- Does not touch or scan the actual data rows (called tuples)
- Very fast, even on large tables

### Internal Process for Each Case

#### 1. `ADD age INT` (Metadata-only)
- Adds new column entry in `pg_attribute` system catalog
- Sets `attmissingval = NULL` for the column
- Leaves actual row data unchanged
- Any read from the `age` column returns `NULL` for existing rows

#### 2. `ADD age INT DEFAULT 24` (Metadata-only)
- Still a metadata-only operation in PostgreSQL 11+
- Default value stored in `pg_attrdef`
- Physical data is not modified for existing rows
- When queried, PostgreSQL returns `NULL` for existing rows
- For new inserts, PostgreSQL injects 24 as default at runtime

#### 3. `ADD age INT DEFAULT 24 NOT NULL` (Table Rewrite)
- PostgreSQL must physically update every row to store the default value 24 on disk
- Rewrites the entire table using a heap rewrite
- Updates visibility maps and all tuples
- High I/O, CPU, and WAL (Write Ahead Log) activity
- Can lock the table for extended periods

## PostgreSQL System Catalogs (Metadata Storage)

### What are System Catalogs?
System catalogs are special tables that store metadata about your database structure. They're regular PostgreSQL tables that are managed internally.

### Important System Catalog Tables

| Table | Purpose |
|-------|---------|
| `pg_class` | Info about tables and indexes |
| `pg_attribute` | Info about columns |
| `pg_type` | Info about data types |
| `pg_namespace` | Info about schemas |
| `pg_constraint` | Info about constraints |
| `pg_index` | Info about indexes |
| `pg_attrdef` | Default values for columns |

### Querying System Catalogs

Check column information:
```sql
SELECT attname, atttypid::regtype AS type, attnotnull, atthasdef
FROM pg_attribute
WHERE attrelid = 'my_table'::regclass AND attnum > 0;
```

Check default values:
```sql
SELECT * FROM pg_attrdef WHERE adrelid = 'my_table'::regclass;
```

### Physical Storage Location

System catalogs are stored on disk in the PostgreSQL data directory:
- Usually at: `/var/lib/postgresql/<version>/main/base/`
- Each database has a numeric ID
- Each table (including catalog tables) is a separate file

Find file locations:
```sql
SELECT relname, relfilenode FROM pg_class WHERE relname = 'pg_attribute';
```

## Best Practice for Large Tables

To avoid downtime when adding columns with default values to large tables:

1. **Add the column first:**
```sql
ALTER TABLE my_table ADD age INT;
```

2. **Batch update in chunks:**
```sql
UPDATE my_table SET age = 24 WHERE age IS NULL LIMIT 100000;
-- Repeat in batches or use a script
```

3. **Add NOT NULL constraint if needed:**
```sql
ALTER TABLE my_table ALTER COLUMN age SET NOT NULL;
```

This staged approach minimizes locking and performance issues.

## Key Terms

| Term | Meaning |
|------|---------|
| **Metadata** | Info about the structure of tables, not the actual data |
| **Metadata-only** | Changes that just update catalogs like `pg_attribute`, not data |
| **pg_attribute** | System catalog that stores metadata about each column in every table |
| **atthasdef** | True if a column has a default value |
| **attnotnull** | True if a column has a NOT NULL constraint |
| **System Catalogs** | Special tables that store database structure information |
